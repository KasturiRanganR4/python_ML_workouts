{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Text classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "UdPxNneN3jVR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Simple TEXT CLASSIFICATION**\n"
      ]
    },
    {
      "metadata": {
        "id": "RsU-tyXE4FWm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**STEP 1**: *Unstructured to Structured Data*"
      ]
    },
    {
      "metadata": {
        "id": "J8lYFFtO33dE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.externals import joblib\n",
        "import pandas, xgboost, numpy, textblob, string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_RUBkhCJiUl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "data = open('/corpus').read()\n",
        "labels, texts = [], []\n",
        "#flag=0\n",
        "for i, line in enumerate(data.split(\"\\n\")):                                      # read the data load it and segregate w.r.t space and divide & assign the content array into two called labels and text.\n",
        "    content = line.split()\n",
        "    labels.append(content[0])\n",
        "    texts.append(\" \".join(content[1:]))\n",
        "    if(i<7):                                                                     # to view the corpus file data before dataframing sample 7\n",
        "      print('Label:', labels[i],\"\\n\",'Text:', content[1:])\n",
        "       \n",
        "'''\n",
        "    if(flag==0):\n",
        "      print('Label:', labels[i],\"\\n\",'Text:', content[1:])\n",
        "      flag=1 \n",
        "'''  \n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "\n",
        "print (\"\\n\",trainDF)                                                             #complete dataframe every row and column matrix format\n",
        "print(\"\\n\",trainDF.iloc[3])                                                      #row vise selection in dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etjzZGJerqGi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the dataset into training and validation datasets randomly >>> x represents text dataframe and y => label dataframe \n",
        "#train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.40, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Training set of text train_x\"\"\\n\")\n",
        "print(train_x.iloc[3])                                                           # to check training set of text\n",
        "print(\"\\n\"\"respective training set of label train_y\"\"\\n\")\n",
        "print(train_y.iloc[3])                                                           # to check the training set of label\n",
        "\n",
        "# label encode the target variable i.e convert the labels into 0 to n-class-1\n",
        "encoder = preprocessing.LabelEncoder()                                           # 'encoder' is the object\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)\n",
        "\n",
        "#encoded results of 7 labels\n",
        "print(train_y[0:7])\n",
        "\n",
        "\n",
        "#In order to reverse the encoded format of the train_y and valid_y\n",
        "\n",
        "#list(encoder.inverse_transform(train_y))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gZHUa74B4mEL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**STEP 2**: ***Feature Selection***"
      ]
    },
    {
      "metadata": {
        "id": "Mu3vXmb9Z1T6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ***Count vectors features***"
      ]
    },
    {
      "metadata": {
        "id": "jI9p8t-Xd681",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a count vectorizer object \n",
        "count_vector_object = CountVectorizer(analyzer='word', token_pattern=r'\\w+')     # word level tokens as per RegExp\n",
        "#xtrain_count =  count_vector_object.fit_transform(train_x)                      # future we will get dimensionality error in classifier\n",
        "#xvalid_count =  count_vector_object.fit_transform(valid_x)\n",
        "\n",
        "count_vector_object.fit(trainDF['text'])                                         # seperate fit and transform can be applied but fit_transform is best\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vector_object.transform(train_x)\n",
        "xvalid_count =  count_vector_object.transform(valid_x)\n",
        "\n",
        "print(count_vector_object.get_feature_names())\n",
        "#print(count_vector_object.get_params())                                         # count vector object parameters\n",
        "print(xtrain_count)                                                              # O/P Details::\n",
        "print(xvalid_count)                                                              # (0[row_number], feature_column_number) frequency_count_of_feature\n",
        "\n",
        "\n",
        "#print(numpy.matrix(xvalid_count))\n",
        "#print(numpy.array(xtrain_count).size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpD8iMOsszNx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**STEP 3: MODEL BUILDING and ACCURACY FINDING**\n"
      ]
    },
    {
      "metadata": {
        "id": "WYiu0N0_3p4W",
        "colab_type": "code",
        "outputId": "f735edf5-69f2-47b1-fb86-48e9ca007715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train_test_model(classifier, feature_vector_train, label, feature_vector_valid, name):\n",
        "                                                                                 # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "                                                                                 # save the trained model in the \".pkl\" file so that objects are serialised into the pickle file.\n",
        "    joblib.dump(classifier, '/' + name + '.pkl') \n",
        "\n",
        "                                                                                 # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    return metrics.accuracy_score(predictions, valid_y)\n",
        "\n",
        "\n",
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_test_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count,'NB-Count')\n",
        "print(\"NB, Count Vectors: \", accuracy*100)\n"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors:  83.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zdFlJeNklFUI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ***Info of  Output ***:\n",
        "**Standard deviation for 20 results = 0.7035.** \n",
        " **Mean value for the 20 results = 83.66**\n",
        "* why average result?\n",
        "dataset division as train and test is random, hence result accuracy changes. average value calculated.\n",
        "\n",
        "**Results of Fixed percentage of test and train set**\n",
        "* Testset =25% and Trainset=75% acc= 83.20%\n",
        "* Testset =30% and Trainset=70% acc= 83.06%\n",
        "* Testset =20% and Trainset=80% acc= 82.89%\n",
        "* Testset =40% and Trainset=60% acc= 83.00%"
      ]
    }
  ]
}