{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Text classification_TF-IDF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw8J0ngW-mcD",
        "colab_type": "text"
      },
      "source": [
        "**Simple TEXT Classification using TF-IDF features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8lYFFtO33dE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.externals import joblib\n",
        "import pandas, xgboost, numpy, textblob, string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_RUBkhCJiUl",
        "colab_type": "code",
        "outputId": "99a4b249-ef02-4874-e02c-3dc68c7d0086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1448
        }
      },
      "source": [
        "# load the dataset\n",
        "data = open('/corpus').read()\n",
        "labels, texts = [], []\n",
        "#flag=0\n",
        "for i, line in enumerate(data.split(\"\\n\")):                                      # read the data load it and segregate w.r.t space and divide & assign the content array into two called labels and text.\n",
        "    content = line.split()\n",
        "    labels.append(content[0])\n",
        "    texts.append(\" \".join(content[1:]))\n",
        "    if(i<7):                                                                     # to view the corpus file data before dataframing sample 7\n",
        "      print('Label:', labels[i],\"\\n\",'Text:', content[1:])\n",
        "       \n",
        "'''\n",
        "    if(flag==0):\n",
        "      print('Label:', labels[i],\"\\n\",'Text:', content[1:])\n",
        "      flag=1 \n",
        "'''  \n",
        "\n",
        "# create a dataframe using texts and lables\n",
        "trainDF = pandas.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "\n",
        "print (\"\\n\",trainDF)                                                             #complete dataframe every row and column matrix format\n",
        "print(\"\\n\",trainDF.iloc[3])                                                      #row vise selection in dataframe"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: __label__2 \n",
            " Text: ['Stuning', 'even', 'for', 'the', 'non-gamer:', 'This', 'sound', 'track', 'was', 'beautiful!', 'It', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'I', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid.', 'game', 'music!', 'I', 'have', 'played', 'the', 'game', 'Chrono', 'Cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'I', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music!', 'It', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras.', 'It', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen!', '^_^']\n",
            "Label: __label__2 \n",
            " Text: ['The', 'best', 'soundtrack', 'ever', 'to', 'anything.:', \"I'm\", 'reading', 'a', 'lot', 'of', 'reviews', 'saying', 'that', 'this', 'is', 'the', 'best', \"'game\", \"soundtrack'\", 'and', 'I', 'figured', 'that', \"I'd\", 'write', 'a', 'review', 'to', 'disagree', 'a', 'bit.', 'This', 'in', 'my', 'opinino', 'is', 'Yasunori', \"Mitsuda's\", 'ultimate', 'masterpiece.', 'The', 'music', 'is', 'timeless', 'and', \"I'm\", 'been', 'listening', 'to', 'it', 'for', 'years', 'now', 'and', 'its', 'beauty', 'simply', 'refuses', 'to', 'fade.The', 'price', 'tag', 'on', 'this', 'is', 'pretty', 'staggering', 'I', 'must', 'say,', 'but', 'if', 'you', 'are', 'going', 'to', 'buy', 'any', 'cd', 'for', 'this', 'much', 'money,', 'this', 'is', 'the', 'only', 'one', 'that', 'I', 'feel', 'would', 'be', 'worth', 'every', 'penny.']\n",
            "Label: __label__2 \n",
            " Text: ['Amazing!:', 'This', 'soundtrack', 'is', 'my', 'favorite', 'music', 'of', 'all', 'time,', 'hands', 'down.', 'The', 'intense', 'sadness', 'of', '\"Prisoners', 'of', 'Fate\"', '(which', 'means', 'all', 'the', 'more', 'if', \"you've\", 'played', 'the', 'game)', 'and', 'the', 'hope', 'in', '\"A', 'Distant', 'Promise\"', 'and', '\"Girl', 'who', 'Stole', 'the', 'Star\"', 'have', 'been', 'an', 'important', 'inspiration', 'to', 'me', 'personally', 'throughout', 'my', 'teen', 'years.', 'The', 'higher', 'energy', 'tracks', 'like', '\"Chrono', 'Cross', '~', \"Time's\", 'Scar~\",', '\"Time', 'of', 'the', 'Dreamwatch\",', 'and', '\"Chronomantique\"', '(indefinably', 'remeniscent', 'of', 'Chrono', 'Trigger)', 'are', 'all', 'absolutely', 'superb', 'as', 'well.This', 'soundtrack', 'is', 'amazing', 'music,', 'probably', 'the', 'best', 'of', 'this', \"composer's\", 'work', '(I', \"haven't\", 'heard', 'the', 'Xenogears', 'soundtrack,', 'so', 'I', \"can't\", 'say', 'for', 'sure),', 'and', 'even', 'if', \"you've\", 'never', 'played', 'the', 'game,', 'it', 'would', 'be', 'worth', 'twice', 'the', 'price', 'to', 'buy', 'it.I', 'wish', 'I', 'could', 'give', 'it', '6', 'stars.']\n",
            "Label: __label__2 \n",
            " Text: ['Excellent', 'Soundtrack:', 'I', 'truly', 'like', 'this', 'soundtrack', 'and', 'I', 'enjoy', 'video', 'game', 'music.', 'I', 'have', 'played', 'this', 'game', 'and', 'most', 'of', 'the', 'music', 'on', 'here', 'I', 'enjoy', 'and', \"it's\", 'truly', 'relaxing', 'and', 'peaceful.On', 'disk', 'one.', 'my', 'favorites', 'are', 'Scars', 'Of', 'Time,', 'Between', 'Life', 'and', 'Death,', 'Forest', 'Of', 'Illusion,', 'Fortress', 'of', 'Ancient', 'Dragons,', 'Lost', 'Fragment,', 'and', 'Drowned', 'Valley.Disk', 'Two:', 'The', 'Draggons,', 'Galdorb', '-', 'Home,', 'Chronomantique,', 'Prisoners', 'of', 'Fate,', 'Gale,', 'and', 'my', 'girlfriend', 'likes', 'ZelbessDisk', 'Three:', 'The', 'best', 'of', 'the', 'three.', 'Garden', 'Of', 'God,', 'Chronopolis,', 'Fates,', 'Jellyfish', 'sea,', 'Burning', 'Orphange,', \"Dragon's\", 'Prayer,', 'Tower', 'Of', 'Stars,', 'Dragon', 'God,', 'and', 'Radical', 'Dreamers', '-', 'Unstealable', 'Jewel.Overall,', 'this', 'is', 'a', 'excellent', 'soundtrack', 'and', 'should', 'be', 'brought', 'by', 'those', 'that', 'like', 'video', 'game', 'music.Xander', 'Cross']\n",
            "Label: __label__2 \n",
            " Text: ['Remember,', 'Pull', 'Your', 'Jaw', 'Off', 'The', 'Floor', 'After', 'Hearing', 'it:', 'If', \"you've\", 'played', 'the', 'game,', 'you', 'know', 'how', 'divine', 'the', 'music', 'is!', 'Every', 'single', 'song', 'tells', 'a', 'story', 'of', 'the', 'game,', \"it's\", 'that', 'good!', 'The', 'greatest', 'songs', 'are', 'without', 'a', 'doubt,', 'Chrono', 'Cross:', \"Time's\", 'Scar,', 'Magical', 'Dreamers:', 'The', 'Wind,', 'The', 'Stars,', 'and', 'the', 'Sea', 'and', 'Radical', 'Dreamers:', 'Unstolen', 'Jewel.', '(Translation', 'varies)', 'This', 'music', 'is', 'perfect', 'if', 'you', 'ask', 'me,', 'the', 'best', 'it', 'can', 'be.', 'Yasunori', 'Mitsuda', 'just', 'poured', 'his', 'heart', 'on', 'and', 'wrote', 'it', 'down', 'on', 'paper.']\n",
            "Label: __label__2 \n",
            " Text: ['an', 'absolute', 'masterpiece:', 'I', 'am', 'quite', 'sure', 'any', 'of', 'you', 'actually', 'taking', 'the', 'time', 'to', 'read', 'this', 'have', 'played', 'the', 'game', 'at', 'least', 'once,', 'and', 'heard', 'at', 'least', 'a', 'few', 'of', 'the', 'tracks', 'here.', 'And', 'whether', 'you', 'were', 'aware', 'of', 'it', 'or', 'not,', \"Mitsuda's\", 'music', 'contributed', 'greatly', 'to', 'the', 'mood', 'of', 'every', 'single', 'minute', 'of', 'the', 'whole', 'game.Composed', 'of', '3', 'CDs', 'and', 'quite', 'a', 'few', 'songs', '(I', \"haven't\", 'an', 'exact', 'count),', 'all', 'of', 'which', 'are', 'heart-rendering', 'and', 'impressively', 'remarkable,', 'this', 'soundtrack', 'is', 'one', 'I', 'assure', 'you', 'you', 'will', 'not', 'forget.', 'It', 'has', 'everything', 'for', 'every', 'listener', '--', 'from', 'fast-paced', 'and', 'energetic', '(Dancing', 'the', 'Tokage', 'or', 'Termina', 'Home),', 'to', 'slower', 'and', 'more', 'haunting', '(Dragon', 'God),', 'to', 'purely', 'beautifully', 'composed', \"(Time's\", 'Scar),', 'to', 'even', 'some', 'fantastic', 'vocals', '(Radical', 'Dreamers).This', 'is', 'one', 'of', 'the', 'best', 'videogame', 'soundtracks', 'out', 'there,', 'and', 'surely', \"Mitsuda's\", 'best', 'ever.', '^_^']\n",
            "Label: __label__1 \n",
            " Text: ['Buyer', 'beware:', 'This', 'is', 'a', 'self-published', 'book,', 'and', 'if', 'you', 'want', 'to', 'know', 'why--read', 'a', 'few', 'paragraphs!', 'Those', '5', 'star', 'reviews', 'must', 'have', 'been', 'written', 'by', 'Ms.', \"Haddon's\", 'family', 'and', 'friends--or', 'perhaps,', 'by', 'herself!', 'I', \"can't\", 'imagine', 'anyone', 'reading', 'the', 'whole', 'thing--I', 'spent', 'an', 'evening', 'with', 'the', 'book', 'and', 'a', 'friend', 'and', 'we', 'were', 'in', 'hysterics', 'reading', 'bits', 'and', 'pieces', 'of', 'it', 'to', 'one', 'another.', 'It', 'is', 'most', 'definitely', 'bad', 'enough', 'to', 'be', 'entered', 'into', 'some', 'kind', 'of', 'a', '\"worst', 'book\"', 'contest.', 'I', \"can't\", 'believe', 'Amazon', 'even', 'sells', 'this', 'kind', 'of', 'thing.', 'Maybe', 'I', 'can', 'offer', 'them', 'my', '8th', 'grade', 'term', 'paper', 'on', '\"To', 'Kill', 'a', 'Mockingbird\"--a', 'book', 'I', 'am', 'quite', 'sure', 'Ms.', 'Haddon', 'never', 'heard', 'of.', 'Anyway,', 'unless', 'you', 'are', 'in', 'a', 'mood', 'to', 'send', 'a', 'book', 'to', 'someone', 'as', 'a', 'joke---stay', 'far,', 'far', 'away', 'from', 'this', 'one!']\n",
            "\n",
            "                                                    text       label\n",
            "0     Stuning even for the non-gamer: This sound tra...  __label__2\n",
            "1     The best soundtrack ever to anything.: I'm rea...  __label__2\n",
            "2     Amazing!: This soundtrack is my favorite music...  __label__2\n",
            "3     Excellent Soundtrack: I truly like this soundt...  __label__2\n",
            "4     Remember, Pull Your Jaw Off The Floor After He...  __label__2\n",
            "5     an absolute masterpiece: I am quite sure any o...  __label__2\n",
            "6     Buyer beware: This is a self-published book, a...  __label__1\n",
            "7     Glorious story: I loved Whisper of the wicked ...  __label__2\n",
            "8     A FIVE STAR BOOK: I just finished reading Whis...  __label__2\n",
            "9     Whispers of the Wicked Saints: This was a easy...  __label__2\n",
            "10    The Worst!: A complete waste of time. Typograp...  __label__1\n",
            "11    Great book: This was a great book,I just could...  __label__2\n",
            "12    Great Read: I thought this book was brilliant,...  __label__2\n",
            "13    Oh please: I guess you have to be a romance no...  __label__1\n",
            "14    Awful beyond belief!: I feel I have to write t...  __label__1\n",
            "15    Don't try to fool us with fake reviews.: It's ...  __label__1\n",
            "16    A romantic zen baseball comedy: When you hear ...  __label__2\n",
            "17    Fashionable Compression Stockings!: After I ha...  __label__2\n",
            "18    Jobst UltraSheer Thigh High: Excellent product...  __label__2\n",
            "19    sizes recomended in the size chart are not rea...  __label__1\n",
            "20    mens ultrasheer: This model may be ok for sede...  __label__1\n",
            "21    Delicious cookie mix: I thought it was funny t...  __label__2\n",
            "22    Another Abysmal Digital Copy: Rather than scra...  __label__1\n",
            "23    A fascinating insight into the life of modern ...  __label__2\n",
            "24    i liked this album more then i thought i would...  __label__2\n",
            "25    Problem with charging smaller AAAs: I have had...  __label__1\n",
            "26    Works, but not as advertised: I bought one of ...  __label__1\n",
            "27    Disappointed: I read the reviews,made my purch...  __label__1\n",
            "28    Oh dear: I was excited to find a book ostensib...  __label__1\n",
            "29    Based on the reviews here I bought one and I'm...  __label__2\n",
            "...                                                 ...         ...\n",
            "9970  beware: The product does give the surround sou...  __label__1\n",
            "9971  happy i only wasted money for 2 Music Experien...  __label__1\n",
            "9972  oh my goodness!: If this is a single release, ...  __label__1\n",
            "9973  The dummy \"FATS\" is hysterical!!!!: ALL I can ...  __label__2\n",
            "9974  Dummy Scared the Be-Jesus Out of Me: Oh, God, ...  __label__1\n",
            "9975  More Ham-O-Rama Theatrics From Sir Anthony: Wh...  __label__1\n",
            "9976  Take The Knife Up The Hill And Rent This Movie...  __label__2\n",
            "9977  MAGIC ADS WERE SCARY!: Though the movie was fr...  __label__2\n",
            "9978  Deliciously disturbing ....Highly Underestimat...  __label__2\n",
            "9979  Magic: If you like Anthony Hopkins, this is on...  __label__2\n",
            "9980  Magic, on Blu Ray, starrring Anthony Hopkins a...  __label__2\n",
            "9981  A ventriloquists nightmare: Magic is a timeles...  __label__2\n",
            "9982  great movie massacred by tape quality: One of ...  __label__1\n",
            "9983  Early Hopkins story still sends chills through...  __label__2\n",
            "9984  The Only Dummy Is The Writer: \"Magic\" poses th...  __label__1\n",
            "9985  \"He's NO Dummy. . .\": Viewing \"Magic\" is when ...  __label__2\n",
            "9986  Amazingly suspenseful psychological thriller: ...  __label__2\n",
            "9987  A truly great horror movie: I saw this film la...  __label__2\n",
            "9988  Frightening movie with superb acting by Sir Ho...  __label__2\n",
            "9989  classic: i got this for my dad. it is super cr...  __label__2\n",
            "9990  Psychological thriller!: This movie really sca...  __label__2\n",
            "9991  A little more money than what I expected to sp...  __label__2\n",
            "9992  \"The Silence of the Dummies\": This is overall ...  __label__1\n",
            "9993  Mauled again - killing bears to enrich himself...  __label__1\n",
            "9994  Sorry Jim: As a former realtor, Mr. Cole owes ...  __label__1\n",
            "9995  A revelation of life in small town America in ...  __label__2\n",
            "9996  Great biography of a very interesting journali...  __label__2\n",
            "9997  Interesting Subject; Poor Presentation: You'd ...  __label__1\n",
            "9998  Don't buy: The box looked used and it is obvio...  __label__1\n",
            "9999  Beautiful Pen and Fast Delivery.: The pen was ...  __label__2\n",
            "\n",
            "[10000 rows x 2 columns]\n",
            "\n",
            " text     Excellent Soundtrack: I truly like this soundt...\n",
            "label                                           __label__2\n",
            "Name: 3, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etjzZGJerqGi",
        "colab_type": "code",
        "outputId": "38504ff5-1e6c-480c-a90c-5a908f85150b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# split the dataset into training and validation datasets randomly >>> x represents text dataframe and y => label dataframe \n",
        "#train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "#train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.40, random_state=32)\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.25, random_state=32, stratify=labels)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Training set of text train_x\"\"\\n\")\n",
        "print(train_x.iloc[3])                                                           # to check training set of text\n",
        "\n",
        "print(\"\\n\"\"respective training set of label train_y\"\"\\n\")\n",
        "print(train_y.iloc[3])                                                           # to check the training set of label\n",
        "\n",
        "# label encode the target variable i.e convert the labels into 0 to n-class-1\n",
        "encoder = preprocessing.LabelEncoder()                                           # 'encoder' is the object\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)\n",
        "\n",
        "#encoded results of 7 labels\n",
        "print(train_y[0:7])\n",
        "\n",
        "\n",
        "#In order to reverse the encoded format of the train_y and valid_y\n",
        "\n",
        "#list(encoder.inverse_transform(train_y))\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set of text train_x\n",
            "\n",
            "This ain't no economics book!: I was expecting a book on economics 101, so maybe I might understand market forces, PE ratios, stuff like that.It's just a big argument for a libertarian government. Not even particularly good arguments either - they're all based on metaphors, like the broken window (paraphrased from memory):'Even though they may get paid, a community does not get rich fixing broken windows. That holds the community still, it does not push it forward.'That's a fine argument and all, but that's not teaching. Metaphors are mental manipulation... give me all the pros and cons and let me decide. Or better: give me the damn facts.And it's outdated too. The book is 50 years old. We are so NOT in Post WWII economics.\n",
            "\n",
            "respective training set of label train_y\n",
            "\n",
            "__label__1\n",
            "[0 1 1 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZHUa74B4mEL",
        "colab_type": "text"
      },
      "source": [
        "**STEP 2**: ***Feature Selection***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu3vXmb9Z1T6",
        "colab_type": "text"
      },
      "source": [
        " ***Tf-Idf features***\n",
        " \n",
        "  **a) word level tf-idf features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI9p8t-Xd681",
        "colab_type": "code",
        "outputId": "a1c64f58-016c-4913-baa0-67f41cba80a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# word level tf-idf\n",
        "\n",
        "tfidf_vector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+', max_features=7000)\n",
        "#tfidf_vector = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+')\n",
        "\n",
        "tfidf_vector.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vector.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vector.transform(valid_x)\n",
        "\n",
        "print(xtrain_tfidf.shape)\n",
        "print(xvalid_tfidf.shape)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6000, 7000)\n",
            "(4000, 7000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mJej8_xGfew",
        "colab_type": "text"
      },
      "source": [
        "**b) n-gram tf-idf features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LqklcdbGQ2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4737a377-0e94-44be-f046-ec7fda04a167"
      },
      "source": [
        "# ngram level tf-idf \n",
        "#tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3))  # accuracy= 49.925, 60:40, stratify\n",
        "\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "print(xtrain_tfidf_ngram.shape)                                      #(7500, 870536) matrix shape without maxfeature\n",
        "print(xvalid_tfidf_ngram.shape)                                      #(2500, 870536)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7500, 870536)\n",
            "(2500, 870536)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpD8iMOsszNx",
        "colab_type": "text"
      },
      "source": [
        "**STEP 3: MODEL BUILDING and ACCURACY FINDING**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYiu0N0_3p4W",
        "colab_type": "code",
        "outputId": "21587507-efdd-4985-b1ff-7883c08b3d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# function to train the model\n",
        "def train_test_model(classifier, feature_vector_train, label, feature_vector_valid, name):\n",
        "                                                                                 # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "                                                                                 # save the trained model in the \".pkl\" file so that objects are serialised into the pickle file.\n",
        "    joblib.dump(classifier, '/' + name + '.pkl') \n",
        "\n",
        "                                                                                 # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    return metrics.accuracy_score(predictions, valid_y)\n",
        "\n",
        "\n",
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_test_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram,'NB-Count')\n",
        "print(\"NB, TFIDF Vectors: \", accuracy*100)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, TFIDF Vectors:  84.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRD7tU-kDSHe",
        "colab_type": "text"
      },
      "source": [
        "# **OUTPUT REVIEWS:**\n",
        "\n",
        "**Dataset: Amazon review dataset (10000 reviews, 2 label)**\n",
        "\n",
        "**word level**\n",
        "**features: 31,666 (without threshold \"maxfeatures\")**\n",
        "**accuracy: 81.35**\n",
        "\n",
        "**features: 7000 (user given maxfeature setting)**\n",
        "**accuracy: 83.375**\n",
        "\n",
        "**n-gram word level**\n",
        "**features: 870536 (without threshold \"maxfeatures\") 70:30**\n",
        "**accuracy: 81.35** "
      ]
    }
  ]
}